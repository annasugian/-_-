# **Проект "Предвзятость ИИ"**
## Термины
- *AI Fairness* - принцип, согласно которому ИИ-системы должны быть созданы и работать таким образом, чтобы они не были предвзяты по отношению к конкретным людям или группам людей.
- *AI Bias* - явление, при котором ИИ выдает предвзятые или несправедливые результаты из-за предвзятости, которая присутствовала в алгоритмах и данных, на которых она была обучена.
- *AI Transparency* относится к возможности понять и объяснить как ИИ принимает решения, и являются ли ее решения справедливыми и этичными.
- *Responsible AI* относится к этической разработке и внедрению ИИ, который приоритетизирует справедливость и человеческое благополучие. Использование ИИ не должно вредить людям.
- *Human-centric AI* - это подход к искусственному интеллекту, который ставит во главу угла человеческое благополучие, ценности и сотрудничество. Он гарантирует, что системы ИИ будут разрабатываться для расширения человеческих возможностей, а не для их замены или контроля.
- *AI Ethics* относится к этическим принципам и гайдлайнам, которые регулируют разработку, развертывание и использование искусственного интеллекта. Она обеспечивает работу систем ИИ справедливым, прозрачным, подотчетным (accountable) и полезным для общества образом.
## Кто этим занимается
## Use-cases
- [Empathy Matters: Embracing Empathy in a Biased World, Unveiling the Complexities of AI Bias and Worldcoin’s Troublesome Debut!!](https://medium.com/@jasminbharadiya/empathy-matters-embracing-empathy-in-a-biased-world-unveiling-the-complexities-of-ai-bias-and-c1c4bfaad9f2) - AI chatbots, like Meta’s BlenderBot, have been caught spreading lies about people. Detecting bias in AI language models like ChatGPT involves analyzing the generated text and identifying patterns that may indicate biased behavior. Authors provide a simplified Python example of how you might approach detecting bias in ChatGPT’s responses, incorporating sentiment analysis, and implementing a more sophisticated scoring system.
- [The Mirror in the Machine: Generative AI, Bias, and the Quest for Fairness](https://medium.com/towards-data-science/the-mirror-in-the-machine-generative-ai-bias-and-the-quest-for-fairness-c39b03a6d48d) - An experiment in generative AI bias for hiring decisions. The results for GPT-3.5 resulted in an astounding preference for the white candidate. The evaluation returned a decision to hire the white candidate 100% of the time. The author fed an AI model with 10 profiles, identical in qualifications and experience, except for the candidates’ names and ethnicities (white and African American). 2 steps: Resume Generation Methods and Results and Resume Evaluation Methods and Results.
- [Evaluating fairness in ChatGPT](https://openai.com/index/evaluating-fairness-in-chatgpt/) - an OpenAI research, where they explored how subtle cues about a user's identity—like their name—can influence ChatGPT's responses. To focus their study on fairness, they looked at whether using names leads to responses that reflect harmful stereotypes. ![image](https://github.com/user-attachments/assets/2f7e6b70-1c5b-40c3-91bd-a37ae19ec398) 
- [How AI reduces the world to stereotypes](https://restofworld.org/2023/ai-image-stereotypes/) - Исследование стереотипов в генерации изображений с помощью ИИ. Авторы анализируют выходные данные моделей, такие как Stable Diffusion и MidJourney, используя качественный и количественный анализ.
- [Let's Do a Thought Experiment: Using Counterfactuals to Improve Moral Reasoning](https://arxiv.org/abs/2306.14308) - Улучшение морального рассуждения в ИИ через контрфактические сценарии. Авторы используют методологию thought experiments и контрфактический анализ для оценки и улучшения этических решений ИИ.
- [Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting](https://arxiv.org/abs/1901.09451) - Исследование предвзятости в биографических данных, используемых для принятия решений. Авторы применяют анализ текстовых данных и семантическое моделирование для выявления и устранения предвзятости.
- [Racial Disparity in Natural Language Processing: A Case Study of Social Media African-American English](https://arxiv.org/abs/1707.00061) - Анализ расовых различий в обработке афроамериканского английского в социальных медиа. Используются методы NLP, такие как токенизация и анализ sentiment, для изучения различий в обработке текста.
- [Towards Effective Discrimination Testing for Generative AI](https://arxiv.org/abs/2412.21052) - Разработка методов тестирования генеративных моделей ИИ на дискриминацию. Авторы предлагают framework для тестирования, включающий симуляции и анализ выходных данных моделей.
- [More is Less? A Simulation-Based Approach to Dynamic Interactions between Biases in Multimodal Models](https://arxiv.org/abs/2412.17505) - Исследование взаимодействия различных типов предвзятости в мультимодальных моделях. Авторы используют симуляции и анализ данных для изучения динамики предвзятости.
- [Diversity and Inclusion in AI for Recruitment: Lessons from Industry Workshop](https://arxiv.org/abs/2411.06066) - Улучшение разнообразия и инклюзивности в ИИ для рекрутинга. Авторы используют данные из индустриального воркшопа и case studies для разработки рекомендаций.
- [Investigating Implicit Bias in Large Language Models: A Large-Scale Study of Over 50 LLMs](https://arxiv.org/abs/2410.12864) - Анализ скрытой предвзятости в более чем 50 крупных языковых моделях. Используются методы тестирования на основе предопределенных сценариев и анализ выходных данных.
- [Responsible AI for Test Equity and Quality: The Duolingo English Test as a Case Study](https://arxiv.org/abs/2409.07476) - Обеспечение справедливости и качества в тестировании с использованием ИИ на примере Duolingo English Test. Анализ данных и тестирование на основе метрик fairness.
- [AI as a Tool for Fair Journalism: Case Studies from Malta](https://arxiv.org/abs/2407.15316) - Использование ИИ для обеспечения справедливости в журналистике. Анализ кейсов из Мальты, используя качественный анализ и case study.
## Методы оценки предвзятости
- [Measuring Bias in AI Models: An Statistical Approach Introducing N-Sigma](https://arxiv.org/pdf/2304.13680) - Метод N-Sigma - статистический подход, используемый для разработки новых систем оценки рисков на основе анализа предвзятости.
- [Justice Or Prejudice? Quantifying Biases In LLM-as-a-Judge](https://arxiv.org/pdf/2410.02736v1) - Разработка системы CALM для автоматизированной количественной оценки предвзятости в LLM-as-a-Judge.
- [GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender Bias in Large Language Models](https://arxiv.org/pdf/2408.12494v1) - Комплексная система оценки и уменьшения гендерных предубеждений - GenderCARE. GenderPair - парный бенчмарк, предназначенный для всесторонней оценки гендерной предвзятости в LLMs.
- [AI Benchmarking Methods For Bias Assessment](https://store-restack.vercel.app/p/ai-benchmarking-answer-benchmarking-methods-ai-bias-cat-ai) - Описание бенчмарков и подходов для анализа и оценки предвзятости.
