# **Проект "Предвзятость ИИ"**
## Термины
- *AI Fairness* - принцип, согласно которому ИИ-системы должны быть созданы и работать таким образом, чтобы они не были предвзяты по отношению к конкретным людям или группам людей.
- *AI Bias* - явление, при котором ИИ выдает предвзятые или несправедливые результаты из-за предвзятости, которая присутствовала в алгоритмах и данных, на которых она была обучена.
- *AI Transparency* относится к возможности понять и объяснить как ИИ принимает решения, и являются ли ее решения справедливыми и этичными.
- *Responsible AI* относится к этической разработке и внедрению ИИ, который приоритетизирует справедливость и человеческое благополучие. Использование ИИ не должно вредить людям.
- *Human-centric AI* - это подход к искусственному интеллекту, который ставит во главу угла человеческое благополучие, ценности и сотрудничество. Он гарантирует, что системы ИИ будут разрабатываться для расширения человеческих возможностей, а не для их замены или контроля.
- *AI Ethics* относится к этическим принципам и гайдлайнам, которые регулируют разработку, развертывание и использование искусственного интеллекта. Она обеспечивает работу систем ИИ справедливым, прозрачным, подотчетным (accountable) и полезным для общества образом.
## Кто этим занимается
## Зачем это надо
### Вроде авторитетные ресерчи
- [The Mirror in the Machine: Generative AI, Bias, and the Quest for Fairness](https://medium.com/towards-data-science/the-mirror-in-the-machine-generative-ai-bias-and-the-quest-for-fairness-c39b03a6d48d) 
 [Medium.com, 2024] - Эксперимент по оценке влияния искусственного интеллекта на принятие решений о приеме на работу. Результаты для GPT-3.5 показали, что предпочтение отдавалось белому кандидату. В результате оценки было принято решение о найме белого кандидата в 100% случаев. Автор ввел в модель искусственного интеллекта 10 профилей, идентичных по квалификации и опыту, за исключением имен и этнической принадлежности кандидатов (белые и афроамериканцы). 2 этапа: Методы и результаты составления резюме и методы и результаты оценки резюме. ![image](https://github.com/user-attachments/assets/65b12744-ea52-4409-9dac-8c2c1aa16c71)

- [Evaluating fairness in ChatGPT](https://openai.com/index/evaluating-fairness-in-chatgpt/) [openai.com, 2024] - В исследовании OpenAI изучалось, как тонкие намеки на личность пользователя, такие как его имя, могут влиять на ответы в ChatGPT. Чтобы сфокусировать свое исследование на справедливости, они изучили, приводит ли использование имен к ответам, отражающим вредные стереотипы. Решений на смягчение предвзятости не было предложено. ![image](https://github.com/user-attachments/assets/285eb319-e570-486f-9d31-f1603c587b6f)

- [How AI reduces the world to stereotypes](https://restofworld.org/2023/ai-image-stereotypes/) [restofworld.com, 2023] - Rest of World проанализировали 3000 изображений с использованием искусственного интеллекта, чтобы увидеть, как генераторы изображений визуализируют разные страны и культуры. Используя Midjourney, они выбрали пять подсказок, основанных на общих понятиях “человек”, “женщина”, “дом”, “улица” и “тарелка с едой”. Затем они адаптировали их для разных стран: Китая, Индии, Индонезии, Мексики и Нигерии. Для сравнения они также включили в исследование США, учитывая, что Midjourney (как и большинство крупнейших компаний, занимающихся генеративным ИИ) базируется в этой стране. Для каждого запроса и сочетания стран (например, “индиец”, “дом в Мексике”, “тарелка нигерийской еды”) они сгенерировали 100 изображений. Результаты имели высокий уровень предвзятости. Авторы связывают это с специфическими данными для обучения модели. ![image](https://github.com/user-attachments/assets/e46fc69e-656a-4ad1-887f-4783e4840663)

- [Mitigating Artificial Intelligence Bias in Financial Systems: A Comparative Analysis of Debiasing Techniques](https://www.researchgate.net/profile/Oluwatofunmi-Oguntibeju/publication/387252070_Mitigating_Artificial_Intelligence_Bias_in_Financial_Systems_A_Comparative_Analysis_of_Debiasing_Techniques/links/6790a8cc98c4e967fa756d43/Mitigating-Artificial-Intelligence-Bias-in-Financial-Systems-A-Comparative-Analysis-of-Debiasing-Techniques.pdf) [researchgate.net, 2025] - Это исследование направлено на продвижение разработки более справедливых систем искусственного интеллекта в банковском секторе, секторе финансовых услуг и страхования (BFSI), предлагая фреймворк FAIR-BIAS. Этот фреймворк обеспечивает структурированный подход к выявлению, смягчению и мониторингу искажений в моделях искусственного интеллекта. Основные рекомендации включают использование равных коэффициентов в качестве показателя справедливости для обеспечения сбалансированных результатов в разных демографических группах, применение методов состязательного анализа во время обучения модели, чтобы свести к минимуму дискриминационные последствия, и проведение регулярных проверок данных. ![image](https://github.com/user-attachments/assets/6ca5befc-d60b-4508-a086-82744c1fc484)

- [Let's Do a Thought Experiment: Using Counterfactuals to Improve Moral Reasoning](https://arxiv.org/abs/2306.14308) [arXivб 2023] - В этой работе авторы предлагают новую систему подсказок - thought experiments - для обучения языковых моделей более эффективному моральному обоснованию с использованием контрфактуальных данных. Результаты экспериментов показывают, что их структура позволяет получать из модели вопросы и ответы, противоречащие фактам, что, в свою очередь, помогает повысить точность выполнения задачи "Моральные сценарии" на 9-16% по сравнению с другими базовыми показателями с нулевым результатом. ![image](https://github.com/user-attachments/assets/fa0a4ef1-e105-4c4c-8d4e-428abd24bfa8)

- [Diversity and Inclusion in AI for Recruitment: Lessons from Industry Workshop](https://arxiv.org/abs/2411.06066) [arXiv, 2024] - Это исследование направлено на изучение практического применения руководящих принципов D&I в системах онлайн-поиска работы, управляемых искусственным интеллектом, в частности, на изучение того, как эти принципы могут быть реализованы на практике для создания более инклюзивных процессов найма. Авторы провели семинар по совместному проектированию с крупной международной рекрутинговой компанией, в ходе которого были рассмотрены два варианта использования ИИ при подборе персонала. Истории пользователей и их личные данные были использованы для оценки влияния ИИ на различные заинтересованные стороны. Полученные результаты предполагают разработку индивидуальных руководящих принципов по разработке и внедрению ИИ и постоянную поддержку для обеспечения эффективного внедрения инклюзивных практик ИИ. ![image](https://github.com/user-attachments/assets/aae7adf6-424f-44b5-bd3e-1581424ec467)

 ### Веб-статьи, за авторитетность которых я не отвечаю
- [What measures does DeepSeek take to prevent AI bias?](https://zilliz.com/ai-faq/what-measures-does-deepseek-take-to-prevent-ai-bias) [ziliz]
- [AI Bias and Safety: Only Fresh & Relevant Examples](https://community.openai.com/t/ai-bias-and-safety-only-fresh-relevant-examples/720680) [OpenAi Developer Community, 2024]
- [Bias Detection and Correction Techniques for OpenAI Models](https://www.signitysolutions.com/tech-insights/openai-bias-detection-correction) [Signity Solutions, 2024]
- [An Analysis of Chinese LLM Censorship and Bias with Qwen 2 Instruct](https://huggingface.co/blog/leonardlin/chinese-llm-censorship-analysis) [Hugging Face, 2024]

## Методы оценки предвзятости
- [Measuring Bias in AI Models: An Statistical Approach Introducing N-Sigma](https://arxiv.org/pdf/2304.13680) - Метод N-Sigma - статистический подход, используемый для разработки новых систем оценки рисков на основе анализа предвзятости.
- [Justice Or Prejudice? Quantifying Biases In LLM-as-a-Judge](https://arxiv.org/pdf/2410.02736v1) - Разработка системы CALM для автоматизированной количественной оценки предвзятости в LLM-as-a-Judge.
- [GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender Bias in Large Language Models](https://arxiv.org/pdf/2408.12494v1) - Комплексная система оценки и уменьшения гендерных предубеждений - GenderCARE. GenderPair - парный бенчмарк, предназначенный для всесторонней оценки гендерной предвзятости в LLMs.
- [AI Benchmarking Methods For Bias Assessment](https://store-restack.vercel.app/p/ai-benchmarking-answer-benchmarking-methods-ai-bias-cat-ai) - Описание бенчмарков и подходов для анализа и оценки предвзятости.
